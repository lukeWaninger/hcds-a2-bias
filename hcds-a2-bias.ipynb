{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:34:55.545390Z",
     "start_time": "2018-10-25T04:34:55.520522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda3/envs/bright/lib/python3.6/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/home/luke/anaconda3/envs/bright/lib/python3.6/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:00.507740Z",
     "start_time": "2018-10-25T04:34:55.548818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda3/envs/bright/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/luke/anaconda3/envs/bright/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Luke Waninger!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import core packages\n",
    "import io\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mul\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# import third party dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import synapseclient\n",
    "from synapseclient import Activity, File\n",
    "\n",
    "# initialize plotly to use offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# adjust the notebook to print everything in the cell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# login to my datastore\n",
    "syn = synapseclient.Synapse()\n",
    "syn.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "Two data sources will be used:\n",
    "To create these tables, we will draw from three data sources:\n",
    "1. Wikipedia Article Data found on [Figshare](https://figshare.com/articles/Untitled_Item/5513449).\n",
    "2. Population Data found at a random [DropBox](https://www.dropbox.com/s/5u7sy1xt7g0oi2c/WPDS_2018_data.csv?dl=0) location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figshare - Wikipedia Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:04.332758Z",
     "start_time": "2018-10-25T04:35:00.519259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Template:ZambiaProvincialMinisters</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>235107991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Template:Zimbabwe-politician-stub</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>391862046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Template:Uganda-politician-stub</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>391862070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Template:Namibia-politician-stub</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>391862409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page   country     rev_id\n",
       "0  Template:ZambiaProvincialMinisters    Zambia  235107991\n",
       "1                      Bir I of Kanem      Chad  355319463\n",
       "2   Template:Zimbabwe-politician-stub  Zimbabwe  391862046\n",
       "3     Template:Uganda-politician-stub    Uganda  391862070\n",
       "4    Template:Namibia-politician-stub   Namibia  391862409"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the data from figshare\n",
    "figshare = 'https://ndownloader.figshare.com/files/9614893'\n",
    "r = requests.get(figshare)\n",
    "\n",
    "# make sure the result is valid\n",
    "if r.ok:\n",
    "    # feed a byte stream into a ZipFile\n",
    "    stream = io.BytesIO(r.content)\n",
    "    zf = ZipFile(stream)\n",
    "    \n",
    "    # locate the csv file within the list of files embedded in the ZipFile generated above\n",
    "    # I make sure to not include the files within the 'MAX OS' directory\n",
    "    file = [\n",
    "        f for f in zf.filelist if f.filename.find('page_data.csv') > 0 and f.filename.find('MAC') == -1\n",
    "    ][0]\n",
    "    \n",
    "    # extract the csv file and read into a pandas dataframe\n",
    "    page_data = pd.read_csv(zf.extract(file))\n",
    "\n",
    "# print this if the request failed for some reason\n",
    "else:\n",
    "    print(f'failed to download page data: {r.status}')\n",
    "\n",
    "page_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population Data - DropBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:04.958381Z",
     "start_time": "2018-10-25T04:35:04.338001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography</th>\n",
       "      <th>Population mid-2018 (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>1,284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Libya</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morocco</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Geography Population mid-2018 (millions)\n",
       "0    AFRICA                          1,284\n",
       "1   Algeria                           42.7\n",
       "2     Egypt                             97\n",
       "3     Libya                            6.5\n",
       "4   Morocco                           35.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the data from Drop Box\n",
    "dropbox = 'https://www.dropbox.com/s/5u7sy1xt7g0oi2c/WPDS_2018_data.csv?dl=1'\n",
    "r = requests.get(dropbox)\n",
    "\n",
    "# make sure the result is valid\n",
    "if r.ok:\n",
    "    # this time, feed the csv byte stream into a pandas dataframe directly\n",
    "    stream = io.BytesIO(r.content)\n",
    "    pop = pd.read_csv(stream)\n",
    "\n",
    "# print this if the request failed for some reason\n",
    "else:\n",
    "    print(f'failed to download population data: {r.status}')\n",
    "\n",
    "pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right at the start we see inconsistencies in the format of our joining keys - Geography and country. I run some quick string tools to ensure they're consistent through both datasets. The columns are renamed in order to make the joining easier to read. Additionally, I verify that the joining keys are, in fact, keys then perform an inner join to include only the countries that have data in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:05.188553Z",
     "start_time": "2018-10-25T04:35:04.962530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "      <th>article_name</th>\n",
       "      <th>revision_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "      <td>Template:Algeria-politician-stub</td>\n",
       "      <td>544347736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "      <td>Template:Algeria-diplomat-stub</td>\n",
       "      <td>567620838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "      <td>Template:AlgerianPres</td>\n",
       "      <td>665948270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "      <td>Ali Fawzi Rebaine</td>\n",
       "      <td>686269631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>42.7</td>\n",
       "      <td>Ahmed Attaf</td>\n",
       "      <td>705910185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  population                      article_name  revision_id\n",
       "0  Algeria        42.7  Template:Algeria-politician-stub    544347736\n",
       "1  Algeria        42.7    Template:Algeria-diplomat-stub    567620838\n",
       "2  Algeria        42.7             Template:AlgerianPres    665948270\n",
       "3  Algeria        42.7                 Ali Fawzi Rebaine    686269631\n",
       "4  Algeria        42.7                       Ahmed Attaf    705910185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explicitly rename the columns\n",
    "pop.rename(columns={\n",
    "    'Geography':'country',\n",
    "    'Population mid-2018 (millions)':'population'\n",
    "}, inplace=True)\n",
    "\n",
    "page_data.rename(columns={\n",
    "    'page':'article_name',\n",
    "    'rev_id':'revision_id'\n",
    "}, inplace=True)\n",
    "\n",
    "# enforce string format consistency\n",
    "pop.country = pop.country.apply(str.title)\n",
    "page_data.country = page_data.country.apply(str.title)\n",
    "\n",
    "# double check the 'keys' in each dataframe are in fact joinable keys\n",
    "assert len(pd.unique(pop.country)) == len(pop)\n",
    "assert len(pd.unique([(t.article_name, t.country) for t in page_data.itertuples()])) == len(page_data)\n",
    "\n",
    "#Q convert the population to a more appropriate data type\n",
    "pop.population = pop.population.apply(lambda x: float(x.replace(',', '')))\n",
    "\n",
    "# merge the data frames\n",
    "df = pop.merge(page_data, on='country', how='inner')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make API calls to get articles predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function we can use to make the API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:05.200954Z",
     "start_time": "2018-10-25T04:35:05.192847Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ores_data(revision_ids):    \n",
    "    # Define the endpoint\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "    \n",
    "    # Specify the parameters - joining the revision IDs together separated by | marks.\n",
    "    params = {\n",
    "        'project': 'enwiki',\n",
    "        'model': 'wp10',\n",
    "        'revids': '|'.join(str(x) for x in revision_ids)\n",
    "    }\n",
    "    \n",
    "    # make the call and verify the response before proceeding\n",
    "    response = requests.get(endpoint.format(**params))\n",
    "    if response.ok:\n",
    "        # convert the response to json\n",
    "        response = response.json()\n",
    "        \n",
    "        # return the scores as a list of tuples, taking only the prediction\n",
    "        results = []\n",
    "        for rid in revision_ids:\n",
    "            # start at the parent node we're interested in\n",
    "            parent = response['enwiki']['scores'][str(rid)]['wp10']\n",
    "\n",
    "            # check for any errors, append either the error or  prediction\n",
    "            if 'error' in parent.keys():\n",
    "                results.append((rid, parent['error']['type']))\n",
    "            else:\n",
    "                results.append((rid, parent['score']['prediction']))\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    # if the request failed, return the status code and list of revision ids so we can retry later\n",
    "    else:\n",
    "        return dict(err=response.status, revision_ids=revision_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the revision ids in batches of 50 to preclude Wikimedia blocking the request. I also run these requests in parallel and cache the results. There are over 45K rows in the dataframe generated above. This turns into a lot of requests so we don't want to make any call more than once and I'd rather not do it iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:35:05.306228Z",
     "start_time": "2018-10-25T04:35:05.205456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped the cell's code and loaded variables results from file '/mnt/c/Users/lukew/OneDrive/School/DATA 512 [HCDS - Ethics]/DATA512_A2/api_results.pkl'.]\n"
     ]
    }
   ],
   "source": [
    "%%cache api_results.pkl results\n",
    "\n",
    "# define how many rev_ids to include in each API call\n",
    "step = 50\n",
    "\n",
    "# start a task pool\n",
    "pool = mul.Pool(mul.cpu_count())\n",
    "\n",
    "# process the calls in parallel\n",
    "results = list(pool.map(\n",
    "    get_ores_data,\n",
    "    [df.revision_id[i:i+step] for i in range(0, len(df), step)]\n",
    "))\n",
    "\n",
    "# make sure to kill the children\n",
    "pool.close(); pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I flatten the list of batched requests into a single list thne convert them to a dataframe and join with the main. Then, I save the file to disk and upload to a proper file store that tracks provenance as a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-25T04:34:55.554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      " Uploading file to Synapse storage \n",
      "##################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flatten the batches into a single list\n",
    "results = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "# convert to a dataframe\n",
    "results = pd.DataFrame(results, columns=['revision_id', 'prediction'])\n",
    "\n",
    "# join with the main\n",
    "df_ = df.merge(results, on='revision_id', how='inner')\n",
    "\n",
    "# verify that each revision id was processed before resetti'ng the variable\n",
    "assert len(df) == len(df_)\n",
    "df = df_; del df_\n",
    "\n",
    "# write out the dataframe and upload to file store with provenance\n",
    "name = 'data-512-a2.csv'\n",
    "df.to_csv(name, index=None)\n",
    "\n",
    "t = syn.setProvenance(\n",
    "    syn.store(File(name=name, path=name, parent='syn17007608')),\n",
    "    activity=Activity(\n",
    "        name='Generate Article Quality Predictions',\n",
    "        description=\\\n",
    "            'The dataset provides the ability to analyse the number and quality ' +\n",
    "            'of politician Wikimedia articles across countries. In this activity, I pull ' +\n",
    "            'two data sources one regarding population data and one with revision ids ' +\n",
    "            'for those articles. I then use the ORES API to get quality predictions for ' +\n",
    "            'article',\n",
    "        used=[\n",
    "            dict(name='Figshare - Wikimedia Articles', url=figshare),\n",
    "            dict(name='DropBox - Population', url=dropbox),\n",
    "            dict(name='ORES', url='https://www.mediawiki.org/wiki/ORES')\n",
    "        ],\n",
    "        executed=[\n",
    "            dict(\n",
    "                name='hcds-a2-bias.ipynb',\n",
    "                url='https://github.com/lukeWaninger/hcds-a2-bias/blob/master/hcds-a2-bias.ipynb'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "); del t\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-25T04:34:55.560Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the population index to make the following computations more readable\n",
    "pop.set_index('country', inplace=True)\n",
    "\n",
    "# group the dataframe by country , count the number of articles in each\n",
    "# additionally, reset the index inorder to collapse the dataframe back\n",
    "# and finally, rename the prediction column to what it now represents\n",
    "table12 = df.loc[:, ['country', 'prediction']]\\\n",
    "    .groupby(['country'])\\\n",
    "    .count()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={'prediction':'num_articles'})\n",
    "\n",
    "# calculate the articles per million as a new column\n",
    "table12['articles_per_million'] = [\n",
    "    np.round(t.num_articles/pop.loc[t.country].population, 1)\n",
    "    for t in table12.itertuples()\n",
    "]\n",
    "\n",
    "# sort the table by the number of articles per million \n",
    "table12 = table12.sort_values(by='articles_per_million', ascending=False)\n",
    "\n",
    "# extract the highest and lowest ten countries into separate dataframes\n",
    "t1 = table12.drop(columns='num_articles').iloc[:10]\n",
    "t2 = table12.drop(columns='num_articles').iloc[-10:]\n",
    "\n",
    "t1; t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-25T04:34:55.565Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, extract only the articles deemed high quality by creating a mask \n",
    "# then, group by country and count the number of articles in each\n",
    "# rename the column to what it now represents\n",
    "# finally, merge with table12 so we have the full count of articles in the same frame\n",
    "table34 = df.loc[[p in ['GA', 'FA'] for p in df.prediction], ['country', 'prediction']]\\\n",
    "    .groupby(['country'])\\\n",
    "    .count()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={'prediction':'num_quality_articles'})\\\n",
    "    .merge(\n",
    "        table12.drop(columns='articles_per_million'),\n",
    "        on='country'\n",
    "    )\n",
    "\n",
    "# calculate the percent of articles deemed high quality as a new column\n",
    "table34['percent_quality'] = [\n",
    "    np.round(t.num_quality_articles/t.num_articles, 3)\n",
    "    for t in table34.itertuples()\n",
    "]\n",
    "\n",
    "# drop the unnecessary columns and sort by the percentage of quality articles\n",
    "table34 = table34.drop(columns=['num_quality_articles', 'num_articles']).sort_values(by='percent_quality', ascending=False)\n",
    "\n",
    "# extract the highest and lowest ten into new dataframes\n",
    "t3 = table34.iloc[:10]\n",
    "t4 = table34.iloc[-10:]\n",
    "\n",
    "t3; t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I visualizate these tables as boxplots to show the extreme skew between countries; just printing a table of the top doesn't give the bias justice. The percentage of articles per million people is skewed so badly that you can only distinguish the outlying countries. The percentage of high quality articles is still quite skewed but not nearly as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-25T04:34:55.573Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the first boxplot\n",
    "h1 = go.Box(\n",
    "    name='.', \n",
    "    x=table12.articles_per_million, \n",
    "    text=table12.country, \n",
    "    boxpoints='all', \n",
    "    jitter=0.3, \n",
    "    xaxis='x1',\n",
    "    marker=dict(color='#2A3D54')\n",
    ")\n",
    "\n",
    "# create the second boxplot\n",
    "h2 = go.Box(\n",
    "    name='.', \n",
    "    x=table34.percent_quality, \n",
    "    text=table34.country,  \n",
    "    boxpoints='all',\n",
    "    jitter=0.3,\n",
    "    xaxis='x2',\n",
    "    marker=dict(color='#B26C10')\n",
    ")\n",
    "\n",
    "# generate a figure containing two subplots\n",
    "fig = tools.make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=[\n",
    "        'Percentage of Articles Per Million People',\n",
    "        'Percentage of Articles Deemed High Quality'\n",
    "    ],\n",
    "    print_grid=False\n",
    ")\n",
    "\n",
    "# append the subplots\n",
    "fig.append_trace(h1, 1, 1)\n",
    "fig.append_trace(h2, 2, 1)\n",
    "\n",
    "# update the layout to not inclue the legend, the titles speak for themselves\n",
    "fig['layout'].update(showlegend=False)\n",
    "\n",
    "# show the figure\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bright]",
   "language": "python",
   "name": "conda-env-bright-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
